{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c16f2c19-3e85-4fc5-9c75-7fec8edda95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Setup\n",
    "\n",
    "Before running this notebook, update the `config.py` file with your catalog and schema names.  \n",
    "A dedicated compute cluster is required to run this notebook:  \n",
    "- Use **Approach 1** with Classic Compute  \n",
    "- Use **Approach 2** with Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7330664f-e46e-4587-8f10-e30fd91dadf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Run this cell to initialize resource names. You can edit resource names in the config file before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765f10e4-1842-4c14-bfec-96032d10bcae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfa1b080-5c57-47d4-b01e-2587fdb785ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write data from CSV to schema to use with AI funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709152f8-3cca-46d8-a06a-8a863bd732f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/synthetic_car_data.csv')\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.synthetic_car_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad489430-4667-4d2d-b2b0-6d6005c1a788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write data from PNG to schema to use with AI funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f5e329-b36e-4ca5-845b-865bd0d6cc42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "data_path = f\"file:/Workspace{current_dir}/data\"\n",
    "display(data_path)\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.`dip`\")\n",
    "dbutils.fs.cp(f\"{data_path}/dip.png/\", f\"/Volumes/{catalog_name}/{schema_name}/dip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade7f86d-29f4-42ec-a0ac-2fcaaa66a615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write data from ZIP file to use with Agent Bricks \n",
    "\n",
    "### Approach #1: Clone and Move Data from GitHub\n",
    "\n",
    "You need a classic compute cluster (not serverless) to move files from the workspace to a UC volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4841b4-6301-47a8-b624-2cc195f7b287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(f\"/Workspace{current_dir}/data/tech_support.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4ada42-2e0b-43ae-b236-2a4ed88cdab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# unzip file\n",
    "with zipfile.ZipFile(f\"/Workspace{current_dir}/data/tech_support.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"/Workspace{current_dir}/data/\")\n",
    "\n",
    "def delete_ds_store_files(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename == '.DS_Store':\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not delete {file_path}: {e}\")\n",
    "\n",
    "root_dir = f\"/Workspace{current_dir}/data/\"\n",
    "delete_ds_store_files(root_dir)\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.`tech_support`\")\n",
    "\n",
    "dbutils.fs.cp(f\"{data_path}/tech_support/\", f\"/Volumes/{catalog_name}/{schema_name}/tech_support\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56fecc90-b013-4c94-96f4-7e9ae971b8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Approach #2: Uploading Data Directly to a Unity Catalog Volume\n",
    "\n",
    "1. Download the Data </br>\n",
    "Download the tech_support.zip archive from the provided GitHub [link](https://github.com/chen-data-ai/Agent-Bricks-Workshop/blob/main/data/tech_support.zip).\n",
    "\n",
    "2. Upload the ZIP File </br>\n",
    "Go to your Databricks workspace.\n",
    "Navigate to your schema’s Unity Catalog volume using one of these methods:\n",
    "\n",
    "    - Sidebar: Select Add data > Upload files to volume\n",
    "    - Catalog Explorer: Click Add > Upload to volume\n",
    "    - Notebook: Select File > Upload the downloaded tech_support.zip file and your catalog or schema information\n",
    "\n",
    "3. Copy the File Path </br>\n",
    "After uploading, locate the file path (e.g., /Volumes/catalog/schema/volume/tech_support.zip) — you’ll need this path for the next step.\n",
    "\n",
    "4. Unzip the File in Databricks </br>\n",
    "Use the following command in a notebook cell to unzip the file to your desired directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aee1af5-c79a-409e-8f6f-0d735e750fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sh unzip '/Volumes/<ADD YOUR CATALOG>/<ADD YOUR SCHEMA>/tech_support/tech_support.zip' -d '/Volumes/<ADD YOUR CATALOG>/<ADD YOUR SCHEMA>/tech_support'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7358320039447319,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "00_Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
